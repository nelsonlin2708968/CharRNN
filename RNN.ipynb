{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Char-RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/karpathy/char-rnn, from Andrej Kaparthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "# import text\n",
    "with open('data/tinyshakespeare.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "    \n",
    "# length of text is the number of characters in it\n",
    "print('Length of text: {} characters'.format(len(text)))\n",
    "\n",
    "# and let's get a glance of what the text is\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file. We use this to build our encoding for the neural network\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n'   --->    0\n",
      "' '    --->    1\n",
      "'!'    --->    2\n",
      "'$'    --->    3\n",
      "'&'    --->    4\n",
      "\"'\"    --->    5\n",
      "','    --->    6\n",
      "'-'    --->    7\n",
      "'.'    --->    8\n",
      "'3'    --->    9\n",
      "':'    --->   10\n",
      "';'    --->   11\n",
      "'?'    --->   12\n",
      "'A'    --->   13\n",
      "'B'    --->   14\n",
      "'C'    --->   15\n",
      "'D'    --->   16\n",
      "'E'    --->   17\n",
      "'F'    --->   18\n",
      "'G'    --->   19\n",
      "First Citi --- characters mapped to int --- > [18 47 56 57 58  1 15 47 58 47]\n"
     ]
    }
   ],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "vocab_to_ind = {c: i for i, c in enumerate(vocab)}\n",
    "ind_to_vocab = dict(enumerate(vocab))\n",
    "text_as_int = np.array([vocab_to_ind[c] for c in text], dtype=np.int32)\n",
    "\n",
    "# We mapped the character as indexes from 0 to len(vocab)\n",
    "for char,_ in zip(vocab_to_ind, range(20)):\n",
    "    print('{:6s} ---> {:4d}'.format(repr(char), vocab_to_ind[char]))\n",
    "# Show how the first 10 characters from the text are mapped to integers\n",
    "print ('{} --- characters mapped to int --- > {}'.format(text[:10], text_as_int[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Creating batches\n",
    "Make generator to yield training batches\n",
    "\n",
    "Let's first clarify the concepts of batches:\n",
    "1. **batch_size**: Reviewing batches in CNN, if we have 100 samples and we set batch_size as 10, it means that we will send 10 samples to the network at one time. In RNN, batch_size have the same meaning, it defines how many samples we send to the network at one time.\n",
    "2. **sequence_length**: However, as for RNN, we store memory in our cells, we pass the information through cells, so we have this sequence_length concept, which also called 'steps', it defines how long a sequence is.\n",
    "\n",
    "From above two concepts, we here clarify the meaning of batch_size in RNN. Here, we define the number of sequences in a batch as N and the length of each sequence as M, so batch_size in RNN **still** represent the number of sequences in a batch but the data size of a batch is actually an array of size **[N, M]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates mini batches\n",
    "def get_batches(array, n_seqs, n_steps):\n",
    "    '''\n",
    "    Partition data array into mini-batches\n",
    "    input: text array\n",
    "    array: input data\n",
    "    n_seqs: number of sequences in a batch\n",
    "    n_steps: length of each sequence\n",
    "    output:\n",
    "    x: inputs\n",
    "    y: targets, which is x with one position shift\n",
    "       you can check the following figure to get the sence of what a target looks like\n",
    "    '''\n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = int(len(array) / batch_size)\n",
    "    # we only keep the full batches and ignore the left.\n",
    "    array = array[:batch_size * n_batches]\n",
    "    array = array.reshape((n_seqs, -1))\n",
    "    \n",
    "    #target array made\n",
    "    target = np.roll(array, shift = -1)\n",
    "    \n",
    "    batch_count = 0\n",
    "    \n",
    "    #print(batch_size, n_batches, array.shape)\n",
    "    while True:\n",
    "        if(batch_count < n_batches):\n",
    "            #print(batch_count)\n",
    "            #print(batch_size)\n",
    "            \n",
    "            #yield array[batch_count * batch_size: (batch_count + 1) * batch_size], target[batch_count : batch_count + 1]\n",
    "            #mprint(array.shape)\n",
    "            yield (array[:,batch_count * n_steps : (batch_count +1) * n_steps], \n",
    "                   target[:,batch_count * n_steps : (batch_count +1) * n_steps]) # yield new batch\n",
    "            batch_count += 1\n",
    "        else:\n",
    "            batch_count = 0\n",
    "        \n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check that words are fed, 10 characters per line, ten lines each. And that y is one step ahead of x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[18 47 56 57 58  1 15 47 58 47]\n",
      " [53  6  1 15 39 47 59 57  1 25]\n",
      " [41 47 59 57 12  0  0 13 50 50]\n",
      " [43 56  1 53 44  1 51 63  1 57]\n",
      " [ 1 57 51 53 58 46 43 56  5 42]\n",
      " [52  1 41 39 50 50 43 42  1 57]\n",
      " [ 1 39 52 57 61 43 56  0 32 46]\n",
      " [61 52  1 61 47 58 46  1 46 47]\n",
      " [58 53  1 56 59 47 52 11  1 50]\n",
      " [ 0 35 39 57  1 52 53 58  1 39]]\n",
      "\n",
      "y\n",
      " [[47 56 57 58  1 15 47 58 47 64]\n",
      " [ 6  1 15 39 47 59 57  1 25 39]\n",
      " [47 59 57 12  0  0 13 50 50 10]\n",
      " [56  1 53 44  1 51 63  1 57 53]\n",
      " [57 51 53 58 46 43 56  5 42  1]\n",
      " [ 1 41 39 50 50 43 42  1 57 53]\n",
      " [39 52 57 61 43 56  0 32 46 43]\n",
      " [52  1 61 47 58 46  1 46 47 51]\n",
      " [53  1 56 59 47 52 11  1 50 43]\n",
      " [35 39 57  1 52 53 58  1 39  1]]\n"
     ]
    }
   ],
   "source": [
    "batch = get_batches(text_as_int, 100, 100)\n",
    "x, y = next(batch)\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Build Char-RNN model\n",
    "In this section, we will build our char-rnn model, it consists of input layer, rnn_cell layer, output layer, loss and optimizer, we will build them one by one.\n",
    "\n",
    "The goal is to predict new text after given prime word, so for our training data, we have to define inputs and targets, here is a figure that explains the structure of the Char-RNN network.\n",
    "\n",
    "![structure](img/charrnn.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-06T23:23:21.549138Z",
     "start_time": "2019-02-06T23:23:21.543724Z"
    }
   },
   "source": [
    "import CharRNN files which contains classes for training and running Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ecbm4040.CharRNN import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Set sampling as False(default), we can start training the network, we automatically save checkpoints in the folder /checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are preset parameters, you can change them to get better result\n",
    "batch_size = 100         # Sequences per batch\n",
    "num_steps = 100          # Number of sequence steps per batch\n",
    "rnn_size = 256           # Size of hidden layers in rnn_cell\n",
    "num_layers = 2           # Number of hidden layers\n",
    "learning_rate = 0.005    # Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 200  loss: 2.2184  0.2252 sec/batch\n",
      "step: 400  loss: 1.9007  0.2337 sec/batch\n",
      "step: 600  loss: 1.7328  0.2348 sec/batch\n",
      "step: 800  loss: 1.6833  0.2295 sec/batch\n",
      "step: 1000  loss: 1.7086  0.2309 sec/batch\n",
      "step: 1200  loss: 1.5561  0.2286 sec/batch\n",
      "step: 1400  loss: 1.5248  0.2322 sec/batch\n",
      "step: 1600  loss: 1.4901  0.2285 sec/batch\n",
      "step: 1800  loss: 1.4342  0.2353 sec/batch\n",
      "step: 2000  loss: 1.4613  0.2281 sec/batch\n",
      "step: 2200  loss: 1.4382  0.2351 sec/batch\n",
      "step: 2400  loss: 1.4122  0.2289 sec/batch\n",
      "step: 2600  loss: 1.4267  0.2340 sec/batch\n",
      "step: 2800  loss: 1.3927  0.2330 sec/batch\n",
      "step: 3000  loss: 1.3689  0.2296 sec/batch\n",
      "step: 3200  loss: 1.4092  0.2293 sec/batch\n",
      "step: 3400  loss: 1.3497  0.2384 sec/batch\n",
      "step: 3600  loss: 1.3855  0.2310 sec/batch\n",
      "step: 3800  loss: 1.3625  0.2305 sec/batch\n",
      "step: 4000  loss: 1.3653  0.2350 sec/batch\n",
      "step: 4200  loss: 1.3310  0.2332 sec/batch\n",
      "step: 4400  loss: 1.3287  0.2258 sec/batch\n",
      "step: 4600  loss: 1.3292  0.2346 sec/batch\n",
      "step: 4800  loss: 1.3455  0.2336 sec/batch\n",
      "step: 5000  loss: 1.3111  0.2261 sec/batch\n",
      "step: 5200  loss: 1.2925  0.2272 sec/batch\n",
      "step: 5400  loss: 1.3142  0.2265 sec/batch\n",
      "step: 5600  loss: 1.3541  0.2315 sec/batch\n",
      "step: 5800  loss: 1.3198  0.2265 sec/batch\n",
      "step: 6000  loss: 1.3012  0.2286 sec/batch\n"
     ]
    }
   ],
   "source": [
    "model = CharRNN(len(vocab), batch_size, num_steps, 'LSTM', rnn_size,\n",
    "               num_layers, learning_rate)\n",
    "batches = get_batches(text_as_int, batch_size, num_steps)\n",
    "model.train(batches = batches, max_count =6000 , save_every_n = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i6000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i2000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i4000_l256.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i6000_l256.ckpt\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up checkpoints\n",
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "Set the sampling as True and we can generate new characters one by one. We can use our saved checkpoints to see how the network learned gradually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24 27 30 16  1]\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/i6000_l256.ckpt\n",
      "24\n",
      "27\n",
      "30\n",
      "16\n",
      "1\n",
      "LORD LEDd RAY:\n",
      "The mine, whose manners would not have the cleacus.\n",
      "I have not shed her what, stroig old. War, womberise!\n",
      "\n",
      "LIONES:\n",
      "To-die, sir, and your good brother,\n",
      "And we are welting anoury'd sorrow so,\n",
      "And he the whict the souse of the markelsher and\n",
      "Wilhink them as, and talk of heavens and well;\n",
      "But I will sorch her the confursorting\n",
      "For that some such a warsing breels fair his\n",
      "And that the hable worss buckle, and should welcome,\n",
      "Would they selon son hath and the stir age wish\n",
      "To pale to hear to him their plantly, womer\n",
      "Is that we warr'd with my made flesh in service\n",
      "To honour the bright thoughty in the strength\n",
      "As to cart thither shall be such tongue.\n",
      "\n",
      "Secord Murderer:\n",
      "I am any trive: both me again this heart;\n",
      "I'll bear me as a time is here and that the\n",
      "servantiss to the ciden there.\n",
      "\n",
      "CAMILLO:\n",
      "What then there is my hard as armine that, that\n",
      "instrument have their that teld you well, that he's strength\n",
      "That troubles the couse o' the sister: who, the mudy sailors; he\n",
      "draws mine own call th\n"
     ]
    }
   ],
   "source": [
    "model = CharRNN(len(vocab), batch_size, num_steps, 'LSTM', rnn_size,\n",
    "               num_layers, learning_rate, sampling=True)\n",
    "\n",
    "# choose the last checkpoint and generate new text\n",
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = model.sample(checkpoint, 1000, len(vocab), vocab_to_ind, ind_to_vocab, prime=\"LORD \")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24 27 30 16  1]\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/i2000_l256.ckpt\n",
      "24\n",
      "27\n",
      "30\n",
      "16\n",
      "1\n",
      "LORD YOLNE:\n",
      "What's thought now think that wall, I more that he\n",
      "shold thy prince shall be behied thither, that I wall shake\n",
      "to\n",
      "do the folior to your sighn there.\n",
      "\n",
      "PAULINA:\n",
      "This hand than that I stay of me, stringsh thou,\n",
      "And mon a suck our senses of this blover,\n",
      "To bud her ttander, and a potth, on holy\n",
      "Would show myself that sermors, and sale,\n",
      "I wI will to his son and so thou ant\n",
      "Twuily the stack and thy person she say is\n",
      "seem off will never mesp this polembent.\n",
      "\n",
      "KING RICHARD II:\n",
      "As shall speak in his face of his heart\n",
      "With him and be a man as sullinger to made my, tears,\n",
      "And stoul a body of the steel of more.\n",
      "\n",
      "KING EDWARDI I:\n",
      "Tenderher best he shall bound mishly sund that\n",
      "To be to hit three talk titn as this sen\n",
      "Where thou wast think the tinder to the pale.\n",
      "\n",
      "LEONTES:\n",
      "Tronoor, and then is hath seel to him.\n",
      "\n",
      "Provost:\n",
      "\n",
      "POTIXANES:\n",
      "Thou shaltst not, but the sair.\n",
      "\n",
      "Shepherd:\n",
      "A strove son my,\n",
      "The soul's persuating of your hands birnies\n",
      "When this is an erolured blist stand,\n",
      "These heast mady set anot\n"
     ]
    }
   ],
   "source": [
    "# choose a checkpoint other than the final one and see the results. It could be nasty, don't worry!\n",
    "#############################################\n",
    "#           TODO: YOUR CODE HERE            #\n",
    "#############################################\n",
    "\n",
    "samp = model.sample(\"checkpoints/i2000_l256.ckpt\", 1000, len(vocab), vocab_to_ind, ind_to_vocab, prime=\"LORD \")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change another type of RNN cell\n",
    "We are using LSTM cell as the original work, but GRU cell is getting more popular today, let's chage the cell in rnn_cell layer to GRU cell and see how it performs. Your number of step should be the same as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: You need to change your saved checkpoints' name or they will rewrite the LSTM results that you have already saved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 200  loss: 1.9804  0.2098 sec/batch\n",
      "step: 400  loss: 1.7614  0.2145 sec/batch\n",
      "step: 600  loss: 1.6194  0.2038 sec/batch\n",
      "step: 800  loss: 1.5851  0.2152 sec/batch\n",
      "step: 1000  loss: 1.6259  0.2144 sec/batch\n",
      "step: 1200  loss: 1.4798  0.2175 sec/batch\n",
      "step: 1400  loss: 1.4599  0.2153 sec/batch\n",
      "step: 1600  loss: 1.4197  0.2084 sec/batch\n",
      "step: 1800  loss: 1.3944  0.2111 sec/batch\n",
      "step: 2000  loss: 1.4177  0.2140 sec/batch\n",
      "step: 2200  loss: 1.4151  0.2123 sec/batch\n",
      "step: 2400  loss: 1.3710  0.2182 sec/batch\n",
      "step: 2600  loss: 1.4120  0.2126 sec/batch\n",
      "step: 2800  loss: 1.3581  0.2099 sec/batch\n",
      "step: 3000  loss: 1.3346  0.2169 sec/batch\n",
      "step: 3200  loss: 1.3833  0.2077 sec/batch\n",
      "step: 3400  loss: 1.3252  0.2092 sec/batch\n",
      "step: 3600  loss: 1.3643  0.2112 sec/batch\n",
      "step: 3800  loss: 1.3473  0.2184 sec/batch\n",
      "step: 4000  loss: 1.3351  0.2086 sec/batch\n",
      "step: 4200  loss: 1.3222  0.2181 sec/batch\n",
      "step: 4400  loss: 1.3226  0.2112 sec/batch\n",
      "step: 4600  loss: 1.3323  0.2094 sec/batch\n",
      "step: 4800  loss: 1.3285  0.2147 sec/batch\n",
      "step: 5000  loss: 1.2893  0.2122 sec/batch\n",
      "step: 5200  loss: 1.2880  0.2106 sec/batch\n",
      "step: 5400  loss: 1.2892  0.2124 sec/batch\n",
      "step: 5600  loss: 1.3378  0.2105 sec/batch\n",
      "step: 5800  loss: 1.3063  0.2091 sec/batch\n",
      "step: 6000  loss: 1.2812  0.2137 sec/batch\n"
     ]
    }
   ],
   "source": [
    "# these are preset parameters, you can change them to get better result\n",
    "batch_size = 100         # Sequences per batch\n",
    "num_steps = 100          # Number of sequence steps per batch\n",
    "rnn_size = 256           # Size of hidden layers in rnn_cell\n",
    "num_layers = 2           # Number of hidden layers\n",
    "learning_rate = 0.005    # Learning rate\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size, num_steps, 'GRU', rnn_size,\n",
    "               num_layers, learning_rate)\n",
    "batches = get_batches(text_as_int, batch_size, num_steps)\n",
    "model.train(batches, 6000, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24 27 30 16  1]\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/i6000_l256.ckpt\n",
      "24\n",
      "27\n",
      "30\n",
      "16\n",
      "1\n",
      "LORD SEAATPAY:\n",
      "The shape, that I must persomed; to be moded\n",
      "A shape whilst yet on thee,, be so like you,\n",
      "That thrusted a wild servant and his sighs.\n",
      "\n",
      "BENVOLIO:\n",
      "Then how now, and a maid to the fiends;\n",
      "To thy heirs but a maidens that your highness\n",
      "Is alwhours being an angel to his:\n",
      "I am warning uver, and more attongue\n",
      "Have those all so sorrow with me that:\n",
      "And how to think you would be so do it,\n",
      "And their presomed, tell what, to mear thee hate,\n",
      "To them be trauss in altermed and treasand,\n",
      "And so betwixt thee, but against the deam,\n",
      "I have allians to think and shall they breatted:\n",
      "I'll be a foul-sound marriage of your bort;\n",
      "That was this will I hope to save his bug?\n",
      "Alack, and the power, teach my trumpets,\n",
      "With troop theict that that the strike that third\n",
      "That you go not to-morrow or a grace,\n",
      "To blend my stiltune for the father\n",
      "She will bring us at all man through them wondred.\n",
      "\n",
      "HORTENSIO:\n",
      "I will peace tale her to his; and I do dint;\n",
      "I'll take your grace to the more and are hither\n",
      "And fire our sh\n"
     ]
    }
   ],
   "source": [
    "model = CharRNN(len(vocab), batch_size, num_steps, 'GRU', rnn_size,\n",
    "               num_layers, learning_rate, sampling=True)\n",
    "# choose the last checkpoint and generate new text\n",
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = model.sample(checkpoint, 1000, len(vocab), vocab_to_ind, ind_to_vocab, prime=\"LORD \")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "1. Compare your result of two networks that you built and the reasons that caused the difference. (It is a qualitative comparison, it should be based on the specific model that you build.)\n",
    "2. Discuss the difference between LSTM cells and GRU cells, what are the pros and cons of using GRU cells?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "1. The GRU cell uses only to two gates, reset and update. Since it doesn't use memory units it trains faster (converges faster to lower loss) and is simpler to build/tweak compared to LSTM. \n",
    "2. GRU doesn't use memory cells, which means they aren't as good at using longer sequence information in theory. However, they are able to train faster and learn better on less training data, while being simpler to tune"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
